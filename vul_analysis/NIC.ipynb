{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model \n",
    "# prepare benign images & adversarial images -> value range should be the same\n",
    "# extract simple NIC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 100\n",
      "(100, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'MNIST_models.CNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.991\n",
      "test acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "from MNIST_models import *\n",
    "import PI\n",
    "\n",
    "meta_params = {\n",
    "    'num_of_train_dataset': 1000,\n",
    "    'num_of_test_dataset': 100,\n",
    "    'is_flatten': False\n",
    "}\n",
    "\n",
    "\n",
    "PI = PI.PIInterface(meta_params)\n",
    "model = load_model('store/MNIST_CNN.pt')\n",
    "PI.set_model(model)\n",
    "print('train acc:', PI.eval_model('train'))\n",
    "print('test acc:', PI.eval_model('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1000 800 200\n",
      "FGSM 813 650 163\n",
      "JSMA 966 772 194\n",
      "CWL2 877 701 176\n",
      "LINFPGD 978 782 196\n",
      "LINFBI 970 776 194\n",
      "ENL1 1000 800 200\n",
      "ST 991 792 199\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "prefix = 'store/'    \n",
    "# LOAD \n",
    "adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "set_of_train_dataset, set_of_test_dataset = [], []\n",
    "\n",
    "for adv_type in adv_types:\n",
    "    # extract from the store file \n",
    "    if adv_type == 'None': fn_name=prefix+'normal.txt'\n",
    "    else: fn_name=prefix+adv_type+'.txt'\n",
    "    fp = open(fn_name, 'rb')\n",
    "    set_of_signatures = pickle.load(fp)\n",
    "    \n",
    "    # separate and store in for later training and evaluation \n",
    "    if adv_type == 'None': split_percentage = 0.8\n",
    "    else: split_percentage = 0.8\n",
    "    split_line = int(len(set_of_signatures)*split_percentage)\n",
    "    train_set_of_signatures, test_set_of_signatures = set_of_signatures[:split_line], set_of_signatures[split_line:]\n",
    "    set_of_train_dataset.append(train_set_of_signatures)\n",
    "    set_of_test_dataset.append(test_set_of_signatures)\n",
    "    fp.close()\n",
    "#     set_of_signatures = np.array(set_of_signatures)\n",
    "#     for i in range(4):\n",
    "#         print(np.max(set_of_signatures[0][i][0]), np.min(set_of_signatures[0][i][0]))\n",
    "    \n",
    "    print(adv_type, len(set_of_signatures), len(train_set_of_signatures), len(test_set_of_signatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1809.939697265625\n",
      "benign correct: 722 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 769 / 772\n",
      "adv ( CWL2 ) correct: 661 / 701\n",
      "adv ( LINFPGD ) correct: 773 / 782\n",
      "adv ( LINFBI ) correct: 745 / 776\n",
      "adv ( ENL1 ) correct: 766 / 800\n",
      "adv ( ST ) correct: 784 / 792\n",
      "acc: 0.9665733574839454\n",
      "benign correct: 180 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 193 / 194\n",
      "adv ( CWL2 ) correct: 152 / 176\n",
      "adv ( LINFPGD ) correct: 176 / 196\n",
      "adv ( LINFBI ) correct: 176 / 194\n",
      "adv ( ENL1 ) correct: 185 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9349540078843627\n",
      "train (acc): 0.9665733574839454\n",
      "test (acc): 0.9349540078843627\n",
      "\n",
      "epoch: 2 loss: 913.9996337890625\n",
      "benign correct: 727 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 771 / 772\n",
      "adv ( CWL2 ) correct: 675 / 701\n",
      "adv ( LINFPGD ) correct: 780 / 782\n",
      "adv ( LINFBI ) correct: 765 / 776\n",
      "adv ( ENL1 ) correct: 786 / 800\n",
      "adv ( ST ) correct: 787 / 792\n",
      "acc: 0.9782644492013832\n",
      "benign correct: 154 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 164 / 176\n",
      "adv ( LINFPGD ) correct: 190 / 196\n",
      "adv ( LINFBI ) correct: 188 / 194\n",
      "adv ( ENL1 ) correct: 194 / 200\n",
      "adv ( ST ) correct: 197 / 199\n",
      "acc: 0.9487516425755584\n",
      "train (acc): 0.9782644492013832\n",
      "test (acc): 0.9487516425755584\n",
      "\n",
      "epoch: 3 loss: 650.23046875\n",
      "benign correct: 758 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 770 / 772\n",
      "adv ( CWL2 ) correct: 675 / 701\n",
      "adv ( LINFPGD ) correct: 780 / 782\n",
      "adv ( LINFBI ) correct: 766 / 776\n",
      "adv ( ENL1 ) correct: 781 / 800\n",
      "adv ( ST ) correct: 784 / 792\n",
      "acc: 0.9820517042647785\n",
      "benign correct: 170 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 166 / 176\n",
      "adv ( LINFPGD ) correct: 188 / 196\n",
      "adv ( LINFBI ) correct: 189 / 194\n",
      "adv ( ENL1 ) correct: 198 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9632063074901446\n",
      "train (acc): 0.9820517042647785\n",
      "test (acc): 0.9632063074901446\n",
      "\n",
      "epoch: 4 loss: 523.3840942382812\n",
      "benign correct: 733 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 771 / 772\n",
      "adv ( CWL2 ) correct: 692 / 701\n",
      "adv ( LINFPGD ) correct: 781 / 782\n",
      "adv ( LINFBI ) correct: 772 / 776\n",
      "adv ( ENL1 ) correct: 796 / 800\n",
      "adv ( ST ) correct: 791 / 792\n",
      "acc: 0.985674296064548\n",
      "benign correct: 163 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 170 / 176\n",
      "adv ( LINFPGD ) correct: 190 / 196\n",
      "adv ( LINFBI ) correct: 192 / 194\n",
      "adv ( ENL1 ) correct: 197 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9638633377135348\n",
      "train (acc): 0.985674296064548\n",
      "test (acc): 0.9638633377135348\n",
      "\n",
      "epoch: 5 loss: 423.0563049316406\n",
      "benign correct: 742 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 694 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 800 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.9892968878643175\n",
      "benign correct: 161 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 170 / 176\n",
      "adv ( LINFPGD ) correct: 192 / 196\n",
      "adv ( LINFBI ) correct: 191 / 194\n",
      "adv ( ENL1 ) correct: 197 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9632063074901446\n",
      "train (acc): 0.9892968878643175\n",
      "test (acc): 0.9632063074901446\n",
      "\n",
      "epoch: 6 loss: 397.4610595703125\n",
      "benign correct: 721 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 692 / 701\n",
      "adv ( LINFPGD ) correct: 780 / 782\n",
      "adv ( LINFBI ) correct: 773 / 776\n",
      "adv ( ENL1 ) correct: 794 / 800\n",
      "adv ( ST ) correct: 791 / 792\n",
      "acc: 0.9835336736374115\n",
      "benign correct: 161 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 169 / 176\n",
      "adv ( LINFPGD ) correct: 188 / 196\n",
      "adv ( LINFBI ) correct: 190 / 194\n",
      "adv ( ENL1 ) correct: 196 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9586070959264126\n",
      "train (acc): 0.9835336736374115\n",
      "test (acc): 0.9586070959264126\n",
      "\n",
      "epoch: 7 loss: 335.0679626464844\n",
      "benign correct: 640 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 699 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 800 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.9733245512926066\n",
      "benign correct: 134 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 174 / 176\n",
      "adv ( LINFPGD ) correct: 194 / 196\n",
      "adv ( LINFBI ) correct: 193 / 194\n",
      "adv ( ENL1 ) correct: 199 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9520367936925098\n",
      "train (acc): 0.9733245512926066\n",
      "test (acc): 0.9520367936925098\n",
      "\n",
      "epoch: 8 loss: 343.0593566894531\n",
      "benign correct: 770 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 771 / 772\n",
      "adv ( CWL2 ) correct: 678 / 701\n",
      "adv ( LINFPGD ) correct: 779 / 782\n",
      "adv ( LINFBI ) correct: 771 / 776\n",
      "adv ( ENL1 ) correct: 784 / 800\n",
      "adv ( ST ) correct: 785 / 792\n",
      "acc: 0.9860036225917997\n",
      "benign correct: 172 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 166 / 176\n",
      "adv ( LINFPGD ) correct: 187 / 196\n",
      "adv ( LINFBI ) correct: 187 / 194\n",
      "adv ( ENL1 ) correct: 192 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9586070959264126\n",
      "train (acc): 0.9860036225917997\n",
      "test (acc): 0.9586070959264126\n",
      "\n",
      "epoch: 9 loss: 330.90386962890625\n",
      "benign correct: 761 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 695 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 797 / 800\n",
      "adv ( ST ) correct: 790 / 792\n",
      "acc: 0.9917668368187057\n",
      "benign correct: 166 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 172 / 176\n",
      "adv ( LINFPGD ) correct: 190 / 196\n",
      "adv ( LINFBI ) correct: 192 / 194\n",
      "adv ( ENL1 ) correct: 198 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9678055190538765\n",
      "train (acc): 0.9917668368187057\n",
      "test (acc): 0.9678055190538765\n",
      "\n",
      "epoch: 10 loss: 316.5298156738281\n",
      "benign correct: 763 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 696 / 701\n",
      "adv ( LINFPGD ) correct: 781 / 782\n",
      "adv ( LINFBI ) correct: 774 / 776\n",
      "adv ( ENL1 ) correct: 799 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.9924254898732093\n",
      "benign correct: 172 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 172 / 176\n",
      "adv ( LINFPGD ) correct: 190 / 196\n",
      "adv ( LINFBI ) correct: 192 / 194\n",
      "adv ( ENL1 ) correct: 198 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9717477003942181\n",
      "train (acc): 0.9924254898732093\n",
      "test (acc): 0.9717477003942181\n",
      "\n",
      "epoch: 11 loss: 266.8028869628906\n",
      "benign correct: 754 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 701 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 800 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.9924254898732093\n",
      "benign correct: 155 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 174 / 176\n",
      "adv ( LINFPGD ) correct: 192 / 196\n",
      "adv ( LINFBI ) correct: 193 / 194\n",
      "adv ( ENL1 ) correct: 199 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9645203679369251\n",
      "train (acc): 0.9924254898732093\n",
      "test (acc): 0.9645203679369251\n",
      "\n",
      "epoch: 12 loss: 248.515380859375\n",
      "benign correct: 769 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 694 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 774 / 776\n",
      "adv ( ENL1 ) correct: 794 / 800\n",
      "adv ( ST ) correct: 790 / 792\n",
      "acc: 0.9920961633459575\n",
      "benign correct: 170 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 170 / 176\n",
      "adv ( LINFPGD ) correct: 189 / 196\n",
      "adv ( LINFBI ) correct: 190 / 194\n",
      "adv ( ENL1 ) correct: 197 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.966491458607096\n",
      "train (acc): 0.9920961633459575\n",
      "test (acc): 0.966491458607096\n",
      "\n",
      "epoch: 13 loss: 255.6553955078125\n",
      "benign correct: 766 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 699 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 775 / 776\n",
      "adv ( ENL1 ) correct: 797 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.9934134694549646\n",
      "benign correct: 166 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv ( CWL2 ) correct: 172 / 176\n",
      "adv ( LINFPGD ) correct: 187 / 196\n",
      "adv ( LINFBI ) correct: 189 / 194\n",
      "adv ( ENL1 ) correct: 198 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9638633377135348\n",
      "train (acc): 0.9934134694549646\n",
      "test (acc): 0.9638633377135348\n",
      "\n",
      "epoch: 14 loss: 266.3631896972656\n",
      "benign correct: 765 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 692 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 793 / 800\n",
      "adv ( ST ) correct: 789 / 792\n",
      "acc: 0.9911081837642022\n",
      "benign correct: 164 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 173 / 176\n",
      "adv ( LINFPGD ) correct: 191 / 196\n",
      "adv ( LINFBI ) correct: 191 / 194\n",
      "adv ( ENL1 ) correct: 199 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9678055190538765\n",
      "train (acc): 0.9911081837642022\n",
      "test (acc): 0.9678055190538765\n",
      "\n",
      "epoch: 15 loss: 252.17510986328125\n",
      "benign correct: 770 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 693 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 775 / 776\n",
      "adv ( ENL1 ) correct: 794 / 800\n",
      "adv ( ST ) correct: 791 / 792\n",
      "acc: 0.9924254898732093\n",
      "benign correct: 172 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 169 / 176\n",
      "adv ( LINFPGD ) correct: 188 / 196\n",
      "adv ( LINFBI ) correct: 187 / 194\n",
      "adv ( ENL1 ) correct: 196 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9638633377135348\n",
      "train (acc): 0.9924254898732093\n",
      "test (acc): 0.9638633377135348\n",
      "\n",
      "epoch: 16 loss: 260.83990478515625\n",
      "benign correct: 771 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 697 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 775 / 776\n",
      "adv ( ENL1 ) correct: 799 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.994236785773094\n",
      "benign correct: 173 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 167 / 176\n",
      "adv ( LINFPGD ) correct: 188 / 196\n",
      "adv ( LINFBI ) correct: 190 / 194\n",
      "adv ( ENL1 ) correct: 195 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9645203679369251\n",
      "train (acc): 0.994236785773094\n",
      "test (acc): 0.9645203679369251\n",
      "\n",
      "epoch: 17 loss: 179.091064453125\n",
      "benign correct: 772 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 684 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 774 / 776\n",
      "adv ( ENL1 ) correct: 786 / 800\n",
      "adv ( ST ) correct: 787 / 792\n",
      "acc: 0.9891322246006916\n",
      "benign correct: 173 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 164 / 176\n",
      "adv ( LINFPGD ) correct: 187 / 196\n",
      "adv ( LINFBI ) correct: 188 / 194\n",
      "adv ( ENL1 ) correct: 192 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9586070959264126\n",
      "train (acc): 0.9891322246006916\n",
      "test (acc): 0.9586070959264126\n",
      "\n",
      "epoch: 18 loss: 206.33572387695312\n",
      "benign correct: 771 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 696 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 797 / 800\n",
      "adv ( ST ) correct: 791 / 792\n",
      "acc: 0.9937427959822164\n",
      "benign correct: 166 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 167 / 176\n",
      "adv ( LINFPGD ) correct: 187 / 196\n",
      "adv ( LINFBI ) correct: 190 / 194\n",
      "adv ( ENL1 ) correct: 196 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9599211563731932\n",
      "train (acc): 0.9937427959822164\n",
      "test (acc): 0.9599211563731932\n",
      "\n",
      "epoch: 19 loss: 225.25033569335938\n",
      "benign correct: 766 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 695 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 793 / 800\n",
      "adv ( ST ) correct: 790 / 792\n",
      "acc: 0.9919315000823317\n",
      "benign correct: 168 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 170 / 176\n",
      "adv ( LINFPGD ) correct: 186 / 196\n",
      "adv ( LINFBI ) correct: 190 / 194\n",
      "adv ( ENL1 ) correct: 197 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9632063074901446\n",
      "train (acc): 0.9919315000823317\n",
      "test (acc): 0.9632063074901446\n",
      "\n",
      "epoch: 20 loss: 222.26182556152344\n",
      "benign correct: 760 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 699 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 799 / 800\n",
      "adv ( ST ) correct: 792 / 792\n",
      "acc: 0.9929194796640869\n",
      "benign correct: 162 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 173 / 176\n",
      "adv ( LINFPGD ) correct: 191 / 196\n",
      "adv ( LINFBI ) correct: 190 / 194\n",
      "adv ( ENL1 ) correct: 199 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9658344283837057\n",
      "train (acc): 0.9929194796640869\n",
      "test (acc): 0.9658344283837057\n",
      "\n",
      "epoch: 21 loss: 227.3011932373047\n",
      "benign correct: 762 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 690 / 701\n",
      "adv ( LINFPGD ) correct: 781 / 782\n",
      "adv ( LINFBI ) correct: 769 / 776\n",
      "adv ( ENL1 ) correct: 792 / 800\n",
      "adv ( ST ) correct: 789 / 792\n",
      "acc: 0.9888028980734398\n",
      "benign correct: 171 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 170 / 176\n",
      "adv ( LINFPGD ) correct: 191 / 196\n",
      "adv ( LINFBI ) correct: 189 / 194\n",
      "adv ( ENL1 ) correct: 197 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9678055190538765\n",
      "train (acc): 0.9888028980734398\n",
      "test (acc): 0.9678055190538765\n",
      "\n",
      "epoch: 22 loss: 217.6757049560547\n",
      "benign correct: 750 / 800\n",
      "adv ( FGSM ) correct: 650 / 650\n",
      "adv ( JSMA ) correct: 772 / 772\n",
      "adv ( CWL2 ) correct: 698 / 701\n",
      "adv ( LINFPGD ) correct: 782 / 782\n",
      "adv ( LINFBI ) correct: 776 / 776\n",
      "adv ( ENL1 ) correct: 798 / 800\n",
      "adv ( ST ) correct: 791 / 792\n",
      "acc: 0.9907788572369505\n",
      "benign correct: 145 / 200\n",
      "adv ( FGSM ) correct: 163 / 163\n",
      "adv ( JSMA ) correct: 194 / 194\n",
      "adv ( CWL2 ) correct: 174 / 176\n",
      "adv ( LINFPGD ) correct: 193 / 196\n",
      "adv ( LINFBI ) correct: 191 / 194\n",
      "adv ( ENL1 ) correct: 198 / 200\n",
      "adv ( ST ) correct: 198 / 199\n",
      "acc: 0.9566360052562418\n",
      "train (acc): 0.9907788572369505\n",
      "test (acc): 0.9566360052562418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class NIC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIC, self).__init__()\n",
    "        n = 128\n",
    "        m = 32\n",
    "\n",
    "        self.fc_compress_1 = nn.Linear(16*24*24, n)\n",
    "        self.fc_compress_2 = nn.Linear(16*10*10, n)\n",
    "        self.fc_compress_3 = nn.Linear(32*3*3, n)\n",
    "        self.fc_compress_4 = nn.Linear(64, n)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output1 = nn.Linear(n, m)\n",
    "        self.output2 = nn.Linear(m, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = x1.view(-1, 16*24*24)\n",
    "        x2 = x2.view(-1, 16*10*10)\n",
    "        x3 = x3.view(-1, 32*3*3)\n",
    "        x4 = x4.view(-1, 64)\n",
    "        \n",
    "        x1 = self.relu(self.fc_compress_1(x1))\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.relu(self.fc_compress_2(x2))\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.relu(self.fc_compress_3(x3))\n",
    "        x3 = self.dropout(x3)\n",
    "        x4 = self.relu(self.fc_compress_4(x4))\n",
    "        x4 = self.dropout(x4)\n",
    "\n",
    "        s = torch.add(x1, x2)\n",
    "        s = torch.add(s, x3)\n",
    "        s = torch.add(s, x4)\n",
    "        s = self.relu(s)\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        s = self.relu(self.output1(s))\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        s = self.output2(s)\n",
    "        \n",
    "        return self.softmax(s)\n",
    "    \n",
    "def test_guard_model(NIC, set_of_test_dataset, adv_types, verbose=False):\n",
    "    NIC.eval()\n",
    "    total_train_correct_count, total_train_count = 0, 0 \n",
    "    for test_dataset, adv_type in zip(set_of_test_dataset, adv_types):\n",
    "        current_count = 0\n",
    "        for singatures in test_dataset:\n",
    "            f1, f2, f3, f4 = preprocess(singatures)\n",
    "            outputs = NIC.forward(f1, f2, f3, f4)\n",
    "            if adv_type == 'None': label = torch.from_numpy(np.array([[1, 0]])).float()\n",
    "            else: label = torch.from_numpy(np.array([[0, 1]])).float()\n",
    "\n",
    "            prediction = (outputs.max(1, keepdim=True)[1]).item()     \n",
    "            if adv_type == 'None': \n",
    "                if (prediction == 0): \n",
    "                    current_count += 1\n",
    "            else: \n",
    "                if (prediction == 1): \n",
    "                    current_count += 1\n",
    "            \n",
    "        # record the current train set acc\n",
    "        if verbose:\n",
    "            if adv_type == 'None': \n",
    "                print('benign correct:', current_count, '/', len(test_dataset))\n",
    "            else:\n",
    "                print('adv (', adv_type, ') correct:', current_count, '/', len(test_dataset))\n",
    "\n",
    "        total_train_correct_count += current_count\n",
    "        total_train_count += len(test_dataset)\n",
    "\n",
    "    acc = total_train_correct_count/total_train_count\n",
    "    if verbose:\n",
    "        print('acc:', acc)\n",
    "        \n",
    "        \n",
    "    NIC.train()\n",
    "    return acc\n",
    "    \n",
    "NIC = NIC()\n",
    "NIC.train()\n",
    "optimizer = torch.optim.Adam(NIC.parameters())\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "epoches = 30\n",
    "\n",
    "train_accs, test_accs, losses = [], [], []\n",
    "set_train_sub_accs, set_test_sub_accs = [], []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    total_loss = None \n",
    "    # labeling ...\n",
    "    train_dataset, train_labels = [], []\n",
    "    for dataset, adv_type in zip(set_of_train_dataset, adv_types):\n",
    "        for singatures in dataset:\n",
    "            if adv_type == 'None': \n",
    "                for _ in range(2):\n",
    "                    train_dataset.append(singatures)\n",
    "                    label = torch.from_numpy(np.array([[1, 0]])).float()\n",
    "                    train_labels.append(label)\n",
    "\n",
    "            else: \n",
    "                train_dataset.append(singatures)\n",
    "                label = torch.from_numpy(np.array([[0, 1]])).float()\n",
    "                train_labels.append(label)\n",
    "\n",
    "    # shuffling \n",
    "    shuffle_indexs = np.arange(len(train_dataset))\n",
    "    np.random.shuffle(shuffle_indexs)\n",
    "\n",
    "    # training \n",
    "    for index in shuffle_indexs:\n",
    "        singatures, label = train_dataset[index], train_labels[index]\n",
    "        f1, f2, f3, f4 = preprocess(singatures)\n",
    "        outputs = NIC.forward(f1, f2, f3, f4)\n",
    "\n",
    "        # for recording the training process \n",
    "        loss = loss_func(outputs, label)\n",
    "        if total_loss is None: total_loss = loss \n",
    "        else: total_loss += loss\n",
    "\n",
    "        # Optimization (back-propogation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch:', (epoch+1), 'loss:', total_loss.item())    \n",
    "    train_acc = test_guard_model(NIC, set_of_train_dataset, adv_types, verbose=True)\n",
    "    test_acc = test_guard_model(NIC, set_of_test_dataset, adv_types, verbose=True)\n",
    "    print('train (acc):', train_acc)\n",
    "    print('test (acc):', test_acc)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
