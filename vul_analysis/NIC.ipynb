{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-requisite  \n",
    "# a. double-check whether all 40000 (5000*8 signatures are succesfully generated) -> checked ... DONE \n",
    "# b. train better v1 and v2 ... \n",
    "\n",
    "# According to the paper of NIC: Detecting Adversarial Samples with Neural Network Invariant Checking\n",
    "# 1.a Train VIs via OSVM (RBF) based on 50000 signatures ... DONE\n",
    "# 1.b Verify v1 to v4 on 35000 (5000*7) signatures ... DONE\n",
    "# 2.a Train reduced models -> obtain signatures from reduced models ... DONE \n",
    "# 2.b Train PIs via OSVM (RBF) based on 50000 signatures ... DONE\n",
    "# 3.  Assemble VIs and PIs together ... DONE \n",
    "# 4.  Evaluate 5000 benign samples + 35000 adversarial samples (5000 per adversarial attack) ... DONE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Accuracy of the network on the 50 test images: 100.000000 %\n",
      "Average confidence of the network on the 50 test images 0.9994149780273438\n",
      " [*] Testing finished!\n",
      "Accuracy of the network on the 50 test images: 98.000000 %\n",
      "Average confidence of the network on the 50 test images 0.9820929718017578\n",
      " [*] Testing finished!\n",
      "Accuracy of the network on the 50 test images: 84.000000 %\n",
      "Average confidence of the network on the 50 test images 0.8173697662353515\n",
      " [*] Testing finished!\n",
      "Accuracy of the network on the 50 test images: 86.000000 %\n",
      "Average confidence of the network on the 50 test images 0.60919677734375\n",
      " [*] Testing finished!\n"
     ]
    }
   ],
   "source": [
    "# 1.a Train VIs via OSVM (RBF) based on 50000 signatures\n",
    "from MNIST_models import *\n",
    "import pickle \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get dataset (V1X to V4X) for training VIs \n",
    "num_of_X = 1000\n",
    "prefixs = ['store_zero/', 'store_one/', 'store_two/', 'store_three/', 'store_four/', \n",
    "        'store_five/', 'store_six/', 'store_seven/', 'store_eight/', 'store_nine/']\n",
    "\n",
    "V1X, V2X, V3X, V4X = [], [], [], []\n",
    "VY = []\n",
    "for i in range(num_of_X):\n",
    "    if (i+1) % 1000 == 0: print(i+1)\n",
    "    for prefix_i, prefix in enumerate(prefixs):\n",
    "        \n",
    "        fn_name = 'store_subs_fadv/'+prefix+'normal'+'_'+str(i+1)+'.txt'\n",
    "        try: fp = open(fn_name, 'rb')\n",
    "        except: continue\n",
    "                \n",
    "        signatures = pickle.load(fp)\n",
    "        f1, f2, f3, f4 = preprocess(signatures)\n",
    "        V1X.append(f1)\n",
    "        V2X.append(f2)\n",
    "        V3X.append(f3)\n",
    "        V4X.append(f4)\n",
    "        VY.append(prefix_i)\n",
    "        \n",
    "        fp.close()\n",
    "\n",
    "# preprocessing \n",
    "V1X, V2X, V3X, V4X = torch.stack(V1X, 0), torch.stack(V2X, 0), torch.stack(V3X, 0), torch.stack(V4X, 0) # list of tensor to tensor\n",
    "V1X, V2X, V3X, V4X = V1X.view(-1, 16*24*24), V2X.view(-1, 16*10*10), V3X.view(-1, 32*3*3), V4X.view(-1, 64) # flatten\n",
    "VY = torch.tensor(VY)\n",
    "\n",
    "from RBF import *\n",
    "args = {\n",
    "    'epoch': 50, \n",
    "    'batch_size': 200, \n",
    "    'lr': 0.05,\n",
    "    'num_class': 10, \n",
    "    'num_centers': 50, # originally 60000 samples to 100 centers -> 10 outputs \n",
    "    'num_of_elements': None,\n",
    "    'save_dir': 'ckpoints',\n",
    "    'model_name': None\n",
    "}\n",
    "\n",
    "# train v1 to v4 & store them for further usage \n",
    "set_VX = [V4X, V3X, V2X, V1X]\n",
    "model_names = ['v4', 'v3', 'v2', 'v1']\n",
    "for i, (VX, model_name) in enumerate(zip(set_VX, model_names)):\n",
    "        \n",
    "    cut_ratio = 0.95\n",
    "    cut_num = int(len(VX)*cut_ratio)\n",
    "    train_VX, test_VX = VX[:cut_num], VX[cut_num:]\n",
    "    train_VY, test_VY = VY[:cut_num], VY[cut_num:]\n",
    "    \n",
    "    args['num_of_elements'] = VX.shape[1]\n",
    "    args['model_name'] = model_name\n",
    "\n",
    "    rbfn = RBFN(args, train_VX, train_VY, test_VX, test_VY)\n",
    "#     rbfn.train()\n",
    "#     rbfn.save()\n",
    "    rbfn.load()\n",
    "    rbfn.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.b Verify v1 to v4 on 35000 adversarial signatures \n",
    "# num_of_X = 500\n",
    "# adv_types = ['FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "# for adv_type in adv_types:\n",
    "#     print()\n",
    "#     print(adv_type)\n",
    "#     V1X, V2X, V3X, V4X = [], [], [], []\n",
    "#     V1Y, V2Y, V3Y, V4Y = [], [], [], []\n",
    "#     for i in range(num_of_X):\n",
    "#         for prefix_i, prefix in enumerate(prefixs):\n",
    "\n",
    "#             fn_name = 'store_subs_fadv/'+prefix+adv_type+'_'+str(i+1)+'.txt'\n",
    "#             try: fp = open(fn_name, 'rb')\n",
    "#             except: continue\n",
    "\n",
    "#             signatures = pickle.load(fp)\n",
    "#             f1, f2, f3, f4 = preprocess(signatures)\n",
    "#             V1X.append(f1)\n",
    "#             V2X.append(f2)\n",
    "#             V3X.append(f3)\n",
    "#             V4X.append(f4)\n",
    "\n",
    "#             V1Y.append(prefix_i)\n",
    "#             V2Y.append(prefix_i)\n",
    "#             V3Y.append(prefix_i)\n",
    "#             V4Y.append(prefix_i)\n",
    "\n",
    "#             fp.close()\n",
    "            \n",
    "#     # preprocessing \n",
    "#     V1X, V2X, V3X, V4X = torch.stack(V1X, 0), torch.stack(V2X, 0), torch.stack(V3X, 0), torch.stack(V4X, 0) # list of tensor to tensor\n",
    "#     V1X, V2X, V3X, V4X = V1X.view(-1, 16*24*24), V2X.view(-1, 16*10*10), V3X.view(-1, 32*3*3), V4X.view(-1, 64) # flatten\n",
    "#     V1Y, V2Y, V3Y, V4Y = torch.tensor(V1Y), torch.tensor(V2Y), torch.tensor(V3Y), torch.tensor(V4Y) # list to tensor\n",
    "\n",
    "#     from RBF import *\n",
    "#     args = {\n",
    "#         'epoch': 30, \n",
    "#         'batch_size': 200, \n",
    "#         'lr': 0.01,\n",
    "#         'num_class': 10, \n",
    "#         'num_centers': 50, # originally 60000 samples to 100 centers -> 10 outputs \n",
    "#         'num_of_elements': None,\n",
    "#         'save_dir': 'ckpoints',\n",
    "#         'model_name': None\n",
    "#     }\n",
    "\n",
    "#     # test v1 to v4 \n",
    "#     set_VX = [V4X, V3X, V2X, V1X]\n",
    "#     set_VY = [V4Y, V3Y, V2Y, V1Y]\n",
    "#     model_names = ['v4', 'v3', 'v2', 'v1']\n",
    "#     for (VX, VY, model_name) in zip(set_VX, set_VY, model_names):\n",
    "#         cut_ratio = 0.8\n",
    "#         cut_num = int(len(VX)*cut_ratio)\n",
    "#         train_VX, test_VX = VX[:cut_num], VX[cut_num:]\n",
    "#         train_VY, test_VY = VY[:cut_num], VY[cut_num:]\n",
    "\n",
    "#         args['num_of_elements'] = VX.shape[1]\n",
    "#         args['model_name'] = model_name\n",
    "\n",
    "#         rbfn = RBFN(args, train_VX, train_VY, test_VX, test_VY)\n",
    "#         rbfn.load()\n",
    "#         rbfn.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a Train reduced models -> obtain signatures from reduced models\n",
    "\n",
    "# train reduced models and get the reduced VX\n",
    "set_reduced_VX = []\n",
    "model_names = ['r4', 'r3', 'r2', 'r1']\n",
    "for (VX, model_name) in zip(set_VX, model_names):\n",
    "    # generate_reduced_model(VX, VY, model_name) # train reduced model \n",
    "    reduced_VX = generate__reduced_signatures_on_reduced_model(VX, model_name)\n",
    "    set_reduced_VX.append(reduced_VX)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 50 test images: 100.000000 %\n",
      "Average confidence of the network on the 50 test images 0.9999959564208984\n",
      " [*] Testing finished!\n",
      "Accuracy of the network on the 50 test images: 100.000000 %\n",
      "Average confidence of the network on the 50 test images 0.9999958038330078\n",
      " [*] Testing finished!\n",
      "Accuracy of the network on the 50 test images: 100.000000 %\n",
      "Average confidence of the network on the 50 test images 0.9999910736083985\n",
      " [*] Testing finished!\n"
     ]
    }
   ],
   "source": [
    "# 2.b Train PIs via OSVM (RBF) based on 50000 signatures\n",
    "reduced_V4X, reduced_V3X, reduced_V2X, reduced_V1X = set_reduced_VX[0], set_reduced_VX[1], set_reduced_VX[2], set_reduced_VX[3]\n",
    "set_reduced_PX = [(reduced_V4X, reduced_V3X), (reduced_V3X, reduced_V2X), (reduced_V2X, reduced_V1X)]\n",
    "model_names = ['p43', 'p32', 'p21']\n",
    "args = {\n",
    "    'epoch': 50, \n",
    "    'batch_size': 200, \n",
    "    'lr': 0.01,\n",
    "    'num_class': 10, \n",
    "    'num_centers': 50, # originally 60000 samples to 100 centers -> 10 outputs \n",
    "    'num_of_elements': None,\n",
    "    'save_dir': 'ckpoints',\n",
    "    'model_name': None\n",
    "}\n",
    "for ((reduced_next_VX, reduced_current_VX), model_name) in zip(set_reduced_PX, model_names):\n",
    "\n",
    "    # combine reduced_next_VX and reduced_current_VX to VX\n",
    "    PY = VY\n",
    "    PX = torch.cat((reduced_current_VX, reduced_next_VX), 1)\n",
    "    \n",
    "    cut_ratio = 0.95\n",
    "    cut_num = int(len(PX)*cut_ratio)\n",
    "    train_PX, test_PX = PX[:cut_num], PX[cut_num:]\n",
    "    train_PY, test_PY = PY[:cut_num], PY[cut_num:]\n",
    "    \n",
    "    args['num_of_elements'] = PX.shape[1]\n",
    "    args['model_name'] = model_name\n",
    "\n",
    "    rbfn = RBFN(args, train_PX, train_PY, test_PX, test_PY)\n",
    "#     rbfn.train()\n",
    "#     rbfn.save()\n",
    "    rbfn.load()\n",
    "    rbfn.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "adv_images/benign0.npy\n",
      "adv_images/benign1.npy\n",
      "adv_images/benign2.npy\n",
      "adv_images/benign3.npy\n",
      "adv_images/benign4.npy\n",
      "adv_images/benign5.npy\n",
      "adv_images/benign6.npy\n",
      "adv_images/benign7.npy\n",
      "adv_images/benign8.npy\n",
      "adv_images/benign9.npy\n",
      "[0.9]\n",
      "FGSM\n",
      "adv_images/FGSM0.npy\n",
      "adv_images/FGSM1.npy\n",
      "adv_images/FGSM2.npy\n",
      "adv_images/FGSM3.npy\n",
      "adv_images/FGSM4.npy\n",
      "adv_images/FGSM5.npy\n",
      "adv_images/FGSM6.npy\n",
      "adv_images/FGSM7.npy\n",
      "adv_images/FGSM8.npy\n",
      "adv_images/FGSM9.npy\n",
      "[0.9, 1.0]\n",
      "JSMA\n",
      "adv_images/JSMA0.npy\n",
      "adv_images/JSMA1.npy\n",
      "adv_images/JSMA2.npy\n",
      "adv_images/JSMA3.npy\n",
      "adv_images/JSMA4.npy\n",
      "adv_images/JSMA5.npy\n",
      "adv_images/JSMA6.npy\n",
      "adv_images/JSMA7.npy\n",
      "adv_images/JSMA8.npy\n",
      "adv_images/JSMA9.npy\n",
      "[0.9, 1.0, 1.0]\n",
      "CWL2\n",
      "adv_images/CWL20.npy\n",
      "adv_images/CWL21.npy\n",
      "adv_images/CWL22.npy\n",
      "adv_images/CWL23.npy\n",
      "adv_images/CWL24.npy\n",
      "adv_images/CWL25.npy\n",
      "adv_images/CWL26.npy\n",
      "adv_images/CWL27.npy\n",
      "adv_images/CWL28.npy\n",
      "adv_images/CWL29.npy\n",
      "[0.9, 1.0, 1.0, 1.0]\n",
      "LINFPGD\n",
      "adv_images/LINFPGD0.npy\n",
      "adv_images/LINFPGD1.npy\n",
      "adv_images/LINFPGD2.npy\n",
      "adv_images/LINFPGD3.npy\n",
      "adv_images/LINFPGD4.npy\n",
      "adv_images/LINFPGD5.npy\n",
      "adv_images/LINFPGD6.npy\n",
      "adv_images/LINFPGD7.npy\n",
      "adv_images/LINFPGD8.npy\n",
      "adv_images/LINFPGD9.npy\n",
      "[0.9, 1.0, 1.0, 1.0, 0.9]\n",
      "LINFBI\n",
      "adv_images/LINFBI0.npy\n",
      "adv_images/LINFBI1.npy\n",
      "adv_images/LINFBI2.npy\n",
      "adv_images/LINFBI3.npy\n",
      "adv_images/LINFBI4.npy\n",
      "adv_images/LINFBI5.npy\n",
      "adv_images/LINFBI6.npy\n",
      "adv_images/LINFBI7.npy\n",
      "adv_images/LINFBI8.npy\n",
      "adv_images/LINFBI9.npy\n",
      "[0.9, 1.0, 1.0, 1.0, 0.9, 0.8]\n",
      "ENL1\n",
      "adv_images/ENL10.npy\n",
      "adv_images/ENL11.npy\n",
      "adv_images/ENL12.npy\n",
      "adv_images/ENL13.npy\n",
      "adv_images/ENL14.npy\n",
      "adv_images/ENL15.npy\n",
      "adv_images/ENL16.npy\n",
      "adv_images/ENL17.npy\n",
      "adv_images/ENL18.npy\n",
      "adv_images/ENL19.npy\n",
      "[0.9, 1.0, 1.0, 1.0, 0.9, 0.8, 1.0]\n",
      "ST\n",
      "adv_images/ST0.npy\n",
      "adv_images/ST1.npy\n",
      "adv_images/ST2.npy\n",
      "adv_images/ST3.npy\n",
      "adv_images/ST4.npy\n",
      "adv_images/ST5.npy\n",
      "adv_images/ST6.npy\n",
      "adv_images/ST7.npy\n",
      "adv_images/ST8.npy\n",
      "adv_images/ST9.npy\n",
      "[0.9, 1.0, 1.0, 1.0, 0.9, 0.8, 1.0, 0.9]\n"
     ]
    }
   ],
   "source": [
    "def NIC_eval(img, label):\n",
    "    score = 1\n",
    "    \n",
    "    # get signatures \n",
    "    f1, f2, f3, f4 = preprocess(img)\n",
    "    f1, f2, f3, f4 = f1.view(-1, 16*24*24), f2.view(-1, 16*10*10), f3.view(-1, 32*3*3), f4.view(-1, 64) # flatten\n",
    "       \n",
    "    args = {\n",
    "    'epoch': 30, \n",
    "    'batch_size': 200, \n",
    "    'lr': 0.01,\n",
    "    'num_class': 10, \n",
    "    'num_centers': 50, # originally 60000 samples to 100 centers -> 10 outputs \n",
    "    'num_of_elements': None,\n",
    "    'save_dir': 'ckpoints',\n",
    "    'model_name': None\n",
    "    }\n",
    "    # get prob from VI \n",
    "    set_VX = [f4, f3, f2, f1]\n",
    "    model_names = ['v4', 'v3', 'v2', 'v1']\n",
    "    for i, (VX, model_name) in enumerate(zip(set_VX, model_names)):\n",
    "        if not (i in [0, 1, 2, 3]): continue \n",
    "            \n",
    "        args['num_of_elements'] = VX.shape[1]\n",
    "        args['model_name'] = model_name\n",
    "\n",
    "        rbfn = RBFN(args, None, None, None, None)\n",
    "        rbfn.load()\n",
    "        p = rbfn.eval(VX, label)\n",
    "        score *= p\n",
    "        \n",
    "    # get prob from PI \n",
    "    set_reduced_VX = []\n",
    "    model_names = ['r4', 'r3', 'r2', 'r1']\n",
    "    for (VX, model_name) in zip(set_VX, model_names):\n",
    "        # generate_reduced_model(VX, VY, model_name) # train reduced model \n",
    "        reduced_VX = generate__reduced_signatures_on_reduced_model(VX, model_name)\n",
    "        set_reduced_VX.append(reduced_VX)\n",
    "        \n",
    "    reduced_V4X, reduced_V3X, reduced_V2X, reduced_V1X = set_reduced_VX[0], set_reduced_VX[1], set_reduced_VX[2], set_reduced_VX[3]\n",
    "    set_reduced_PX = [(reduced_V4X, reduced_V3X), (reduced_V3X, reduced_V2X), (reduced_V2X, reduced_V1X)]\n",
    "    model_names = ['p43', 'p32', 'p21']\n",
    "    args = {\n",
    "        'epoch': 30, \n",
    "        'batch_size': 200, \n",
    "        'lr': 0.01,\n",
    "        'num_class': 10, \n",
    "        'num_centers': 50, # originally 60000 samples to 100 centers -> 10 outputs \n",
    "        'num_of_elements': None,\n",
    "        'save_dir': 'ckpoints',\n",
    "        'model_name': None\n",
    "    }\n",
    "    \n",
    "    for i, ((reduced_next_VX, reduced_current_VX), model_name) in enumerate(zip(set_reduced_PX, model_names)):\n",
    "        if not (i in [0, 1, 2]): continue \n",
    "        \n",
    "        # combine reduced_next_VX and reduced_current_VX to VX\n",
    "        PY = VY\n",
    "        PX = torch.cat((reduced_current_VX, reduced_next_VX), 1)\n",
    "\n",
    "        args['num_of_elements'] = PX.shape[1]\n",
    "        args['model_name'] = model_name\n",
    "\n",
    "        rbfn = RBFN(args, None, None, None, None)\n",
    "        rbfn.load()\n",
    "        p = rbfn.eval(PX, label)\n",
    "        score *= p    \n",
    "        \n",
    "    # compute joint probs \n",
    "    threshold = 0.15\n",
    "    if score > threshold: \n",
    "        return True \n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "from LP_utils import *\n",
    "model = load_model('store/MNIST_CNN.pt')\n",
    "prefixs = ['store_zero/', 'store_one/', 'store_two/', 'store_three/', 'store_four/', \n",
    "        'store_five/', 'store_six/', 'store_seven/', 'store_eight/', 'store_nine/']\n",
    "\n",
    "num_of_test_imgs = 10\n",
    "adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "accs = []\n",
    "for adv_type in adv_types:\n",
    "    print(adv_type)\n",
    "    acc = 0\n",
    "    total = 0 \n",
    "    for i in range(num_of_test_imgs):\n",
    "        if adv_type ==  'None': \n",
    "            # Load  \n",
    "            fn = 'adv_images/'+'benign'+str(i)+'.npy'\n",
    "            print(fn)\n",
    "            x = np.load(fn)\n",
    "            \n",
    "            data = torch.from_numpy(np.expand_dims(x, axis=0).astype(np.float32))\n",
    "            outputs = model.forward(data).detach().numpy()\n",
    "            prediction = np.argmax(outputs, axis=1)\n",
    "            singatures = extract_signature_from_CNN(model, x)\n",
    "            if NIC_eval(singatures, prediction): acc += 1\n",
    "            total += 1\n",
    "\n",
    "        elif not (adv_type is None): \n",
    "            fn = 'adv_images/'+adv_type+str(i)+'.npy'\n",
    "            print(fn)\n",
    "            try: adv_x = np.load(fn)\n",
    "            except: continue\n",
    "                \n",
    "            data = torch.from_numpy(np.expand_dims(adv_x, axis=0).astype(np.float32))\n",
    "            outputs = model.forward(data).detach().numpy()\n",
    "            prediction = np.argmax(outputs, axis=1)[0]\n",
    "            singatures = extract_signature_from_CNN(model, adv_x)\n",
    "            if not NIC_eval(singatures, prediction): acc += 1\n",
    "            total += 1\n",
    "                \n",
    "                \n",
    "    accs.append(acc/total)\n",
    "    print(accs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 0.9\n",
      "FGSM 1.0\n",
      "JSMA 1.0\n",
      "CWL2 1.0\n",
      "LINFPGD 0.9\n",
      "LINFBI 0.8\n",
      "ENL1 1.0\n",
      "ST 0.9\n"
     ]
    }
   ],
   "source": [
    "for adv_type, acc in zip(adv_types, accs):\n",
    "    print(adv_type, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
