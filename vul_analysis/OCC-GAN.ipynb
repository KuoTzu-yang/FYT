{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 100\n",
      "(100, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'MNIST_models.CNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "import PI\n",
    "meta_params = {\n",
    "    'num_of_train_dataset': 1000,\n",
    "    'num_of_test_dataset': 100,\n",
    "    'is_flatten': False\n",
    "}\n",
    "\n",
    "PI = PI.PIInterface(meta_params)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from MNIST_models import *\n",
    "\n",
    "model = load_model('store/MNIST_CNN.pt')\n",
    "PI.set_model(model)\n",
    "# print('train acc:', PI.eval_model('train'))\n",
    "# print('test acc:', PI.eval_model('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Each convolutional layer is followed by batch normalization (BN) \n",
    "and activation function (ReLU for generator and Leaky ReLU for discriminator)\n",
    "\n",
    "For Generator:\n",
    "\n",
    "For Discriminator: softmax + MSE(||softmax(D(x))-label(one hot)||^2)\n",
    "\n",
    "\n",
    "Implemented:\n",
    "shuffle=True (unsure)\n",
    "mini-batch size: 256\n",
    "Inputs 32 * 32 (padding 2)\n",
    "concat layer for dense block in generator \n",
    "softmax for discriminator \n",
    "\n",
    "Pending:\n",
    "draw samples during training \n",
    "compute acc \n",
    "'''\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# create loader with mini-batch size 256 and padding images to 32*32\n",
    "trans = transforms.Compose([\n",
    "    transforms.Pad(padding=2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset=torchvision.datasets.MNIST(root='.data', train=True, transform=trans, download=True)\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuotzuyang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:115: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Gen loss: 0.8 Dis loss: 0.529 Dis (DX) loss: 0.329 Dis (DGX) loss: 0.2\n",
      "2 Gen loss: 0.773 Dis loss: 0.523 Dis (DX) loss: 0.295 Dis (DGX) loss: 0.227\n",
      "3 Gen loss: 0.722 Dis loss: 0.521 Dis (DX) loss: 0.243 Dis (DGX) loss: 0.278\n",
      "4 Gen loss: 0.711 Dis loss: 0.521 Dis (DX) loss: 0.232 Dis (DGX) loss: 0.289\n",
      "5 Gen loss: 0.708 Dis loss: 0.523 Dis (DX) loss: 0.231 Dis (DGX) loss: 0.292\n",
      "6 Gen loss: 0.717 Dis loss: 0.522 Dis (DX) loss: 0.239 Dis (DGX) loss: 0.283\n",
      "7 Gen loss: 0.717 Dis loss: 0.519 Dis (DX) loss: 0.236 Dis (DGX) loss: 0.283\n",
      "8 Gen loss: 0.749 Dis loss: 0.519 Dis (DX) loss: 0.267 Dis (DGX) loss: 0.251\n",
      "9 Gen loss: 0.733 Dis loss: 0.521 Dis (DX) loss: 0.253 Dis (DGX) loss: 0.267\n",
      "10 Gen loss: 0.753 Dis loss: 0.52 Dis (DX) loss: 0.273 Dis (DGX) loss: 0.247\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_of_features):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # input layer\n",
    "        self.input_deconv = nn.ConvTranspose2d(1, num_of_features, 4)\n",
    "        \n",
    "        # dense block \n",
    "        self.dense_conv1 = nn.Conv2d(num_of_features, num_of_features, 1)\n",
    "        self.dense_conv2 = nn.Conv2d(num_of_features, num_of_features, 3, padding=1)\n",
    "        self.dense_bn = nn.BatchNorm2d(num_of_features)\n",
    "        \n",
    "        # transition layer\n",
    "        self.trans_conv = nn.Conv2d(num_of_features, num_of_features, 1)\n",
    "        self.trans_deconv = nn.ConvTranspose2d(num_of_features, num_of_features, 2, stride=2)\n",
    "        self.trans_bn = nn.BatchNorm2d(num_of_features)\n",
    "        \n",
    "        # output layer \n",
    "        self.output_conv = nn.Conv2d(num_of_features, 1, 1)\n",
    "        self.output_bn = nn.BatchNorm2d(1)\n",
    "        \n",
    "    def dense_block(self, x):\n",
    "        x2 = self.dense_conv1(x)\n",
    "        x2 = self.relu(self.dense_bn(x2))\n",
    "        x3 = self.dense_conv2(x2)\n",
    "        x3 = self.relu(self.dense_bn(x3))\n",
    "        return x3 + x\n",
    "    \n",
    "    def transition_layer(self, x):\n",
    "        x2 = self.trans_conv(x)\n",
    "        x2 = self.relu(self.trans_bn(x2))\n",
    "        x3 = self.trans_deconv(x2)\n",
    "        return x3\n",
    "    \n",
    "    def output_layer(self, x):\n",
    "        x2 = self.output_conv(x)\n",
    "        x2 = self.relu(self.output_bn(x2))\n",
    "        return x2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_deconv(x)\n",
    "        \n",
    "        x = self.dense_block(x)\n",
    "        x = self.transition_layer(x)\n",
    "        \n",
    "        x = self.dense_block(x)\n",
    "        x = self.transition_layer(x)\n",
    "        \n",
    "        x = self.dense_block(x)\n",
    "        x = self.transition_layer(x)\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        return x \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_of_features):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        # input layer\n",
    "        self.input_conv = nn.Conv2d(1, num_of_features, 1)\n",
    "        self.input_bn = nn.BatchNorm2d(num_of_features)\n",
    "        \n",
    "        # dense block \n",
    "        self.dense_conv1 = nn.Conv2d(num_of_features, num_of_features, 1)\n",
    "        self.dense_conv2 = nn.Conv2d(num_of_features, num_of_features, 3, padding=1)\n",
    "        self.dense_bn = nn.BatchNorm2d(num_of_features)\n",
    "        \n",
    "        # transition layer\n",
    "        self.trans_conv = nn.Conv2d(num_of_features, num_of_features, 1)\n",
    "        self.trans_pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.trans_bn = nn.BatchNorm2d(num_of_features)\n",
    "        \n",
    "        # output layer \n",
    "        self.output_fc = nn.Linear(32*4*4, 2)\n",
    "    \n",
    "    def input_layer(self, x):\n",
    "        x2 = self.input_conv(x)\n",
    "        x2 = self.relu(self.input_bn(x2))\n",
    "        return x2\n",
    "    \n",
    "    def dense_block(self, x):\n",
    "        x2 = self.dense_conv1(x)\n",
    "        x2 = self.relu(self.dense_bn(x2))\n",
    "        x3 = self.dense_conv2(x2)\n",
    "        x3 = self.relu(self.dense_bn(x3))\n",
    "        return x3 + x\n",
    "    \n",
    "    def transition_layer(self, x):\n",
    "        x2 = self.trans_conv(x)\n",
    "        x2 = self.relu(self.trans_bn(x2))\n",
    "        x3 = self.trans_pool(x2)\n",
    "        return x3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        x = self.dense_block(x)\n",
    "        x = self.transition_layer(x)\n",
    "\n",
    "        x = self.dense_block(x)\n",
    "        x = self.transition_layer(x)\n",
    "        \n",
    "        x = self.dense_block(x)\n",
    "        x = self.transition_layer(x)\n",
    "        \n",
    "        x = x.view(-1, 32*4*4)\n",
    "        x = self.output_fc(x)\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "num_of_features = 32\n",
    "draw_interval = 5\n",
    "G = Generator(num_of_features)\n",
    "D = Discriminator(num_of_features)\n",
    "\n",
    "G_optim = torch.optim.Adam(G.parameters(), lr=1e-3)\n",
    "D_optim = torch.optim.Adam(D.parameters(), lr=1e-4)\n",
    "loss_func = nn.MSELoss()\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# pretrained G \n",
    "# pending \n",
    "\n",
    "# Game between G and D \n",
    "for i, (X, Y) in enumerate(loader):\n",
    "    if i < 10:\n",
    "        noises = torch.randn((256, 1, 1, 1))\n",
    "        G_X = G(noises)\n",
    "\n",
    "        S_D_X = D(X)\n",
    "        labels = torch.zeros(S_D_X.shape)\n",
    "        labels[:, 0] = 1.\n",
    "        DX_loss = loss_func(S_D_X, labels)\n",
    "\n",
    "        S_D_GX = D(G_X)\n",
    "        labels = torch.zeros(S_D_GX.shape)\n",
    "        labels[:, 1] = 1.\n",
    "        DGX_loss = loss_func(S_D_X, labels)\n",
    "\n",
    "        D_loss = DX_loss + DGX_loss\n",
    "        D_optim.zero_grad()\n",
    "        D_loss.backward(retain_graph=True)\n",
    "        D_optim.step()\n",
    "\n",
    "        G_optim.zero_grad()\n",
    "        G_loss = 1 - DGX_loss\n",
    "        G_loss.backward()\n",
    "        G_optim.step()\n",
    "\n",
    "        print(i+1, 'Gen loss:', round(G_loss.item(), 3), 'Dis loss:', round(D_loss.item(), 3), 'Dis (DX) loss:', round(DX_loss.item(), 3), 'Dis (DGX) loss:', round(DGX_loss.item(), 3))\n",
    "\n",
    "#         if i % draw_interval == 0:\n",
    "#             img_counter = 1\n",
    "#             img_bound = 4\n",
    "\n",
    "#             for x in X:\n",
    "#                 x = x[:, 2:-2, 2:-2]\n",
    "#                 img = x.reshape(28, 28)\n",
    "                \n",
    "#                 plt.subplot(2, 2, img_counter)\n",
    "#                 img_counter += 1\n",
    "                \n",
    "#                 if img_counter == img_bound: break \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
