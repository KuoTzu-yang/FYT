{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model \n",
    "# prepare benign images & adversarial images -> value range should be the same\n",
    "# extract simple NIC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n",
      "(1000, 1, 28, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmake\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'MNIST_models.CNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\nmake\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\nmake\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.991\n",
      "test acc: 0.974\n"
     ]
    }
   ],
   "source": [
    "from MNIST_models import *\n",
    "import PI\n",
    "\n",
    "meta_params = {\n",
    "    'num_of_train_dataset': 1000,\n",
    "    'num_of_test_dataset': 1000,\n",
    "    'is_flatten': False\n",
    "}\n",
    "\n",
    "\n",
    "PI = PI.PIInterface(meta_params)\n",
    "model = load_model('store/MNIST_CNN.pt')\n",
    "PI.set_model(model)\n",
    "print('train acc:', PI.eval_model('train'))\n",
    "print('test acc:', PI.eval_model('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1000 800 200\n",
      "FGSM 813 650 163\n",
      "JSMA 966 772 194\n",
      "CWL2 877 701 176\n",
      "LINFPGD 978 782 196\n",
      "LINFBI 970 776 194\n",
      "ENL1 1000 800 200\n",
      "ST 991 792 199\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "prefix = 'store/'    \n",
    "# LOAD \n",
    "adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "set_of_train_dataset, set_of_test_dataset = [], []\n",
    "\n",
    "for adv_type in adv_types:\n",
    "    # extract from the store file \n",
    "    if adv_type == 'None': fn_name=prefix+'normal.txt'\n",
    "    else: fn_name=prefix+adv_type+'.txt'\n",
    "    fp = open(fn_name, 'rb')\n",
    "    set_of_signatures = pickle.load(fp)\n",
    "    \n",
    "    # separate and store in for later training and evaluation \n",
    "    if adv_type == 'None': split_percentage = 0.8\n",
    "    else: split_percentage = 0.8\n",
    "    split_line = int(len(set_of_signatures)*split_percentage)\n",
    "    train_set_of_signatures, test_set_of_signatures = set_of_signatures[:split_line], set_of_signatures[split_line:]\n",
    "    set_of_train_dataset.append(train_set_of_signatures)\n",
    "    set_of_test_dataset.append(test_set_of_signatures)\n",
    "    fp.close()\n",
    "#     set_of_signatures = np.array(set_of_signatures)\n",
    "#     for i in range(4):\n",
    "#         print(np.max(set_of_signatures[0][i][0]), np.min(set_of_signatures[0][i][0]))\n",
    "    \n",
    "    print(adv_type, len(set_of_signatures), len(train_set_of_signatures), len(test_set_of_signatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class NIC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NIC, self).__init__()\n",
    "        n = 16\n",
    "        m = 2\n",
    "\n",
    "        self.fc_compress_1 = nn.Linear(16*24*24, n)\n",
    "        self.fc_compress_2 = nn.Linear(16*10*10, n)\n",
    "        self.fc_compress_3 = nn.Linear(32*3*3, n)\n",
    "        self.fc_compress_4 = nn.Linear(64, n)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output = nn.Linear(n, m)\n",
    "        \n",
    "    def forward(self, x1, x2, x3, x4):\n",
    "        x1 = x1.view(-1, 16*24*24)\n",
    "        x2 = x2.view(-1, 16*10*10)\n",
    "        x3 = x3.view(-1, 32*3*3)\n",
    "        x4 = x4.view(-1, 64)\n",
    "        \n",
    "        x1 = self.relu(self.fc_compress_1(x1))\n",
    "        x2 = self.relu(self.fc_compress_2(x2))\n",
    "        x3 = self.relu(self.fc_compress_3(x3))\n",
    "        x4 = self.relu(self.fc_compress_4(x4))\n",
    "        \n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x3 = self.dropout(x3)\n",
    "        x4 = self.dropout(x4)\n",
    "\n",
    "        s = torch.add(x1, x2)\n",
    "        \n",
    "        s = self.relu(s)\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        s = torch.add(s, x3)\n",
    "        \n",
    "        s = self.relu(s)\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        s = torch.add(s, x4)\n",
    "        \n",
    "        s = self.relu(s)\n",
    "        s = self.dropout(s)\n",
    "        \n",
    "        s = self.relu(self.output(s))\n",
    "        \n",
    "        return self.softmax(s)\n",
    "    \n",
    "def test_guard_model(nic, set_of_test_dataset, adv_types, verbose=False):\n",
    "    nic.eval()\n",
    "    total_train_correct_count, total_train_count = 0, 0 \n",
    "    for test_dataset, adv_type in zip(set_of_test_dataset, adv_types):\n",
    "        current_count = 0\n",
    "        for singatures in test_dataset:\n",
    "            f1, f2, f3, f4 = preprocess(singatures)\n",
    "            outputs = nic.forward(f1, f2, f3, f4)\n",
    "            if adv_type == 'None': label = torch.from_numpy(np.array([[1, 0]])).float()\n",
    "            else: label = torch.from_numpy(np.array([[0, 1]])).float()\n",
    "\n",
    "            prediction = (outputs.max(1, keepdim=True)[1]).item()     \n",
    "            if adv_type == 'None': \n",
    "                if (prediction == 0): \n",
    "                    current_count += 1\n",
    "            else: \n",
    "                if (prediction == 1): \n",
    "                    current_count += 1\n",
    "            \n",
    "        # record the current train set acc\n",
    "        if verbose:\n",
    "            if adv_type == 'None': \n",
    "                print('benign correct:', current_count, '/', len(test_dataset))\n",
    "            else:\n",
    "                print('adv (', adv_type, ') correct:', current_count, '/', len(test_dataset))\n",
    "\n",
    "        total_train_correct_count += current_count\n",
    "        total_train_count += len(test_dataset)\n",
    "\n",
    "    acc = total_train_correct_count/total_train_count\n",
    "    if verbose:\n",
    "        print('acc:', acc)\n",
    "        \n",
    "        \n",
    "    nic.train()\n",
    "    return acc\n",
    "    \n",
    "nic = NIC()\n",
    "nic.train()\n",
    "optimizer = torch.optim.Adam(nic.parameters())\n",
    "loss_func = nn.BCELoss()\n",
    "epoches = 0\n",
    "\n",
    "train_accs, test_accs, losses = [], [], []\n",
    "set_train_sub_accs, set_test_sub_accs = [], []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    total_loss = None \n",
    "    # labeling ...\n",
    "    train_dataset, train_labels = [], []\n",
    "    for dataset, adv_type in zip(set_of_train_dataset, adv_types):\n",
    "        for singatures in dataset:\n",
    "            if adv_type == 'None': \n",
    "                for _ in range(1):\n",
    "                    train_dataset.append(singatures)\n",
    "                    label = torch.from_numpy(np.array([[1, 0]])).float()\n",
    "                    train_labels.append(label)\n",
    "\n",
    "            else: \n",
    "                train_dataset.append(singatures)\n",
    "                label = torch.from_numpy(np.array([[0, 1]])).float()\n",
    "                train_labels.append(label)\n",
    "\n",
    "    # shuffling \n",
    "    shuffle_indexs = np.arange(len(train_dataset))\n",
    "    np.random.shuffle(shuffle_indexs)\n",
    "\n",
    "    # training \n",
    "    for index in shuffle_indexs:\n",
    "        singatures, label = train_dataset[index], train_labels[index]\n",
    "        f1, f2, f3, f4 = preprocess(singatures)\n",
    "        outputs = nic.forward(f1, f2, f3, f4)\n",
    "\n",
    "        # for recording the training process \n",
    "        loss = loss_func(outputs, label) \n",
    "        if total_loss is None: total_loss = loss \n",
    "        else: total_loss += loss\n",
    "\n",
    "        # Optimization (back-propogation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch:', (epoch+1), 'loss:', total_loss.item())    \n",
    "    train_acc = test_guard_model(nic, set_of_train_dataset, adv_types, verbose=True)\n",
    "    test_acc = test_guard_model(nic, set_of_test_dataset, adv_types, verbose=True)\n",
    "    print('train (acc):', train_acc)\n",
    "    print('test (acc):', test_acc)\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE & STORE\n",
    "# adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "# for adv_type in adv_types:\n",
    "#     if adv_type == 'None': \n",
    "#         set_of_signatures = PI.generate_signatures() \n",
    "#     else: \n",
    "#         fn_name='store/'+adv_type+'.txt'\n",
    "#         set_of_signatures = PI.generate_signatures(adv_type=adv_type)\n",
    "\n",
    "# LOAD\n",
    "prefixs = ['store_zero/', 'store_one/', 'store_two/', 'store_three/', 'store_four/', \n",
    "        'store_five/', 'store_six/', 'store_seven/', 'store_eight/', 'store_nine/']    \n",
    "\n",
    "one_to_nine_set_of_train_dataset = []\n",
    "one_to_nine_set_of_test_dataset = []\n",
    "\n",
    "for idx, prefix in enumerate(prefixs):\n",
    "    fn = 'store_subs_fadv/'+prefix+'guard.pt'\n",
    "    \n",
    "    set_of_train_dataset = []\n",
    "    set_of_test_dataset = []\n",
    "    \n",
    "    # LOAD DATASET \n",
    "    adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "    set_of_train_dataset, set_of_test_dataset = [], []\n",
    "    for adv_type in adv_types:\n",
    "        \n",
    "        set_of_signatures = []\n",
    "        for i in range(1000):\n",
    "            # extract from the store file \n",
    "            if adv_type == 'None': fn_name = 'store_subs_fadv/'+prefix+'normal'+'_'+str(i+1)+'.txt'\n",
    "            else: fn_name = 'store_subs_fadv/'+prefix+adv_type+'_'+str(i+1)+'.txt'\n",
    "            try: \n",
    "                fp = open(fn_name, 'rb')\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            signatures = pickle.load(fp)\n",
    "            fp.close()\n",
    "            set_of_signatures.append(signatures)\n",
    "\n",
    "        # separate and store in for later training and evaluation \n",
    "        if adv_type == 'None': split_percentage = 0.8\n",
    "        else: split_percentage = 0.8\n",
    "        split_line = int(len(set_of_signatures)*split_percentage)\n",
    "        \n",
    "        train_set_of_signatures, test_set_of_signatures = set_of_signatures[:split_line], set_of_signatures[split_line:]\n",
    "        set_of_train_dataset.append(train_set_of_signatures)\n",
    "        set_of_test_dataset.append(test_set_of_signatures)\n",
    "        # print(prefix, adv_type, len(set_of_signatures), len(train_set_of_signatures), len(test_set_of_signatures))\n",
    "    \n",
    "    one_to_nine_set_of_train_dataset.append(set_of_train_dataset)\n",
    "    one_to_nine_set_of_test_dataset.append(set_of_test_dataset)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "\n",
    "# figsize = (12, 10)\n",
    "# fig=plt.figure(figsize=figsize)\n",
    "\n",
    "# adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "# for i in range(10):\n",
    "#     fig.add_subplot(5, 2, i+1)\n",
    "#     x = []\n",
    "#     for adv_i, adv_type in enumerate(adv_types):\n",
    "#         x.append(len(one_to_nine_set_of_train_dataset[i][adv_i]))\n",
    "    \n",
    "#     index = np.arange(len(x))\n",
    "#     plt.bar(index, x, align='center', alpha=0.5)\n",
    "#     plt.xticks(index, adv_types)\n",
    "#     plt.title(i)\n",
    "#     plt.ylim([0, 350])\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "# plt.savefig('adv_samples_dis_per_class.jpg')\n",
    "# plt.show()\n",
    "\n",
    "# prefixs = ['store_zero/', 'store_one/', 'store_two/', 'store_three/', 'store_four/', \n",
    "#         'store_five/', 'store_six/', 'store_seven/', 'store_eight/', 'store_nine/']  \n",
    "\n",
    "# for idx, prefix in enumerate(prefixs):\n",
    "#     if not (idx in [2, 4, 9]): continue\n",
    "        \n",
    "#     fn = 'store_subs_fadv/'+prefix+'nic.pt'\n",
    "    \n",
    "#     # LOAD DATASET \n",
    "#     set_of_train_dataset = one_to_nine_set_of_train_dataset[idx]\n",
    "#     set_of_test_dataset = one_to_nine_set_of_test_dataset[idx]\n",
    "\n",
    "#     #####################################################################\n",
    "#     # TRAIN\n",
    "#     #####################################################################\n",
    "#     nic = NIC()\n",
    "#     nic.train()\n",
    "#     optimizer = torch.optim.Adam(nic.parameters(), lr=1e-4)\n",
    "#     loss_func = nn.MSELoss()\n",
    "#     epoches = 50\n",
    "\n",
    "#     train_accs, test_accs, losses = [], [], []\n",
    "\n",
    "#     for epoch in range(epoches):\n",
    "#         total_loss = None \n",
    "#         # labeling ...\n",
    "#         train_dataset, train_labels = [], []\n",
    "#         for dataset, adv_type in zip(set_of_train_dataset, adv_types):\n",
    "#             for singatures in dataset:\n",
    "#                 if adv_type == 'None': \n",
    "#                     for _ in range(1):\n",
    "#                         train_dataset.append(singatures)\n",
    "#                         label = torch.from_numpy(np.array([[1, 0]])).float()\n",
    "#                         train_labels.append(label)\n",
    "\n",
    "#                 else: \n",
    "#                     train_dataset.append(singatures)\n",
    "#                     label = torch.from_numpy(np.array([[0, 1]])).float()\n",
    "#                     train_labels.append(label)\n",
    "\n",
    "#         # shuffling \n",
    "#         shuffle_indexs = np.arange(len(train_dataset))\n",
    "#         np.random.shuffle(shuffle_indexs)\n",
    "\n",
    "#         # training \n",
    "#         for index in shuffle_indexs:\n",
    "#             singatures, label = train_dataset[index], train_labels[index]\n",
    "#             f1, f2, f3, f4 = preprocess(singatures)\n",
    "#             outputs = nic.forward(f1, f2, f3, f4)\n",
    "\n",
    "#             # for recording the training process \n",
    "#             loss = loss_func(outputs, label)\n",
    "#             # print(round(loss.item(), 5), round(l1.item(), 5), round(l2.item(), 5))\n",
    "#             if total_loss is None: total_loss = loss \n",
    "#             else: total_loss += loss\n",
    "\n",
    "#             # Optimization (back-propogation)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # record training process \n",
    "#         print(fn)\n",
    "#         print('epoch:', (epoch+1), 'loss:', total_loss.item())    \n",
    "#         train_acc = test_guard_model(nic, set_of_train_dataset, adv_types, verbose=False)\n",
    "#         test_acc = test_guard_model(nic, set_of_test_dataset, adv_types, verbose=False)\n",
    "#         print('train (acc):', train_acc)\n",
    "#         print('test (acc):', test_acc)\n",
    "#         print()\n",
    "        \n",
    "        \n",
    "#         train_accs.append(train_acc)\n",
    "#         test_accs.append(test_acc)\n",
    "#         losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "#     # plot the training process \n",
    "#     fig.add_subplot(2, 5, idx+1)\n",
    "#     plt.plot(np.arange(len(losses)), losses/np.max(np.array(losses)))\n",
    "#     plt.plot(np.arange(len(train_accs)), train_accs)\n",
    "#     plt.plot(np.arange(len(test_accs)), test_accs)\n",
    "#     plt.title(idx)\n",
    "#     plt.ylim([0, 1.05])\n",
    "#     plt.legend(['loss', 'train acc', 'test acc'], loc='lower left')\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     nic.eval()\n",
    "#     torch.save(nic, fn)\n",
    "\n",
    "# # plt.savefig('zero_to_nine_ONIC_train_process_2.jpg')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmake\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct_count: 825 total_count: 974\n",
      "acc: 0.8470225872689938\n",
      "0.8470225872689938\n",
      "FGSM\n",
      "correct_count: 816 total_count: 816\n",
      "acc: 1.0\n",
      "1.0\n",
      "JSMA\n",
      "correct_count: 970 total_count: 973\n",
      "acc: 0.9969167523124358\n",
      "0.9969167523124358\n",
      "CWL2\n",
      "correct_count: 850 total_count: 860\n",
      "acc: 0.9883720930232558\n",
      "0.9883720930232558\n",
      "LINFPGD\n",
      "correct_count: 967 total_count: 973\n",
      "acc: 0.9938335046248715\n",
      "0.9938335046248715\n",
      "LINFBI\n",
      "correct_count: 941 total_count: 957\n",
      "acc: 0.9832810867293625\n",
      "0.9832810867293625\n",
      "ENL1\n",
      "correct_count: 990 total_count: 1000\n",
      "acc: 0.99\n",
      "0.99\n",
      "ST\n",
      "correct_count: 874 total_count: 882\n",
      "acc: 0.9909297052154195\n",
      "0.9909297052154195\n"
     ]
    }
   ],
   "source": [
    "prefixs = ['store_zero/', 'store_one/', 'store_two/', 'store_three/', 'store_four/', \n",
    "           'store_five/', 'store_six/', 'store_seven/', 'store_eight/', 'store_nine/']     \n",
    "\n",
    "# LOAD \n",
    "guards = []\n",
    "for idx, prefix in enumerate(prefixs):\n",
    "    # LOAD TRAINED SUB-GUARD\n",
    "    guard_model = torch.load('store_subs_fadv/'+prefix+'nic.pt')\n",
    "    guards.append(guard_model)\n",
    "    \n",
    "adv_types = ['None', 'FGSM', 'JSMA', 'CWL2', 'LINFPGD', 'LINFBI', 'ENL1', 'ST']\n",
    "accs = []\n",
    "for adv_type in adv_types:\n",
    "    print(adv_type)\n",
    "    if adv_type == 'None':\n",
    "        acc = PI.eval_sub_guards(guards)\n",
    "        accs.append(acc)        \n",
    "    else:\n",
    "        acc = PI.eval_sub_guards(guards, adv_type)\n",
    "        accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figsize = (8, 4)\n",
    "fig=plt.figure(figsize=figsize)\n",
    "x = accs\n",
    "index = np.arange(len(x))\n",
    "plt.bar(index, x, align='center', color='cyyyyyyy')\n",
    "plt.xticks(index, adv_types)\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "plt.axhline(y=1.0, color='r', linestyle='--')\n",
    "plt.savefig('eval_assemble_ONIC.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
